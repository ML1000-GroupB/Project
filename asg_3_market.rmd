---
title: "ML1000 Assignment 3"
author: "Anupama r.k, Queenie Tsang, Crystal (Yunan) Zhu"
date: "09/03/2021"
output:
  pdf_document: default
  html_document: default
  always_allow_html: true
---




## Abstract

The main goal of recommender systems is to provide suggestions to online users to make better decisions from many alternatives available over the Web. A better recommender system is directed more towards personalized recommendations by taking into consideration information about a product, such as specifications, users' purchase history, comparison with other products, and so on, before making recommendations.

## Business Case



## Data Understanding

Instacart Market Basket Analysis dataset obtained from https://www.kaggle.com/c/instacart-market-basket-analysis/data (https://www.kaggle.com/c/instacart-market-basket-analysis/data)

### How do we merge the data files?


There are six data files, excluding the sample_submission.csv file, from the Instacart Market Basket Analysis data - aisles.csv, departments.csv, order_products__train.csv, order_products__prior.csv, orders.csv and products.csv.

### Data file descriptions

aisles.csv - contains aisle id and aisle description columns

departments.csv - contains department id and department description columns

order_products__*.csv - These files specify which products were purchased in each order. order_products__prior.csv contains previous order contents for all customers. 'reordered' indicates that the customer has a previous order that contains the product. order_products_train.csv contains order information for transactions which will be used for training the model.

orders.csv - This file tells to which set (prior, train, test) an order belongs. You are predicting reordered items only for the test set orders. 'order_dow' is the day of week.

products.csv - contains product_id, product_name, aisle_id, department_id


### Steps to merge the data files:

1. Merged the aisles data with the products data to obtain Merged dataset 1, so that we know which aisle each product belongs to.

2. Combined the Merged dataset 1 with the department data to obtain Merged dataset 2, so we know which aisle and department each product is from.

3. Add Merged dataset 2, which contains product full information, to order_products__train and order_products__prior files, respectively, to obtain Merged dataset 3 (Train) and Merged dataset 4 (Prior), so that we know the product information (e.g. product names, aisles and departments they belong to) of the products in the training and prior orders.




```{r, echo=FALSE, warning=FALSE, message=FALSE}
library(tidyr)
library(dplyr)
library(arules)    #provides the infrastructure for representing, manipulating and analyzing transaction data and patterns (frequent itemsets and  #association rules)
library(arulesViz)
library(tidyverse)
library(ggplot2)
library(lubridate)  #work with date and time data
library(RColorBrewer)
library(magrittr)
library(knitr)
library(recommenderlab)
library(data.table)

library(tibble)
library(rsparse)
library(Matrix)
library(Metrics)



```

Read in the merged training data set:
```{r}
X <- read.csv("C:/Users/qt09n/Desktop/Project/orders_TRAIN_products_MERGED.csv")
```

```{r}
str(X)
```
```{r}
dim(X)
```

Check missing values: complete.cases will return a logical vector indicating which rows have no missing values. Then use the vector
to get only complete rows with X[,]
```{r}
X <- X[complete.cases(X),]

```

```{r}
sum(is.na(X))

```
No missing values in dataset.

look at first 10 rows of dataset
```{r}
head(X)
```

```{r}
summary(X)
```

Check total users:
```{r}
user_count <- unique(X$user_id)
length(user_count)
```

Change the character columns to factors and create a new column using mutate:
```{r}
X$product_name <- as.factor(X$product_name)
X$department<- as.factor(X$department)
X$aisle <- as.factor(X$aisle)
```

```{r}
unique(X$aisle)
#134 aisles
```
```{r}
unique(X$department)
levels(X$department)
#21 departments
```
```{r}
length(unique(X$product_name))
#39123 products
```
Total products from product id column:
```{r}
product_count <- unique(X$product_id)
length(product_count)
```

```{r}
class(X$order_hour_of_day)
#[1] "integer"
```

How many unique orders are in the training dataset?
```{r}
length(unique(X$order_id))
```

How many users are in the training dataset?
```{r}
length(unique(X$user_id))
```
So it looks like the number of users are the same as the number of orders...


Recode order_hour_of_day to numeric:
```{r}
X$order_hour_of_day <- as.numeric(X$order_hour_of_day)
```

Look at when people order:
```{r}
X %>% ggplot(aes(x= order_hour_of_day)) +
  geom_bar(stat="count", fill="green")
```
People mostly order from 8:00 - 17:00 (8AM - 5PM).


Number of distinct orders by user:
```{r}
p1<-X %>% 
  group_by(user_id) %>% 
  dplyr::summarise(count_order=n()) %>% 
  ungroup() 

ggplot(p1, aes(count_order)) + geom_histogram(binwidth = 10)+labs(title="Number of distinct orders by user",
        x ="Number of Orders", y = "Number of Users")+coord_cartesian(xlim = c(0, 80))
```

Number of distinct products per user:
```{r, warnings=FALSE}
p2 <- X %>%
  group_by(user_id, product_id) %>%
  dplyr::summarise(count3=n()) %>%
  select(user_id, product_id, count3) %>%
  ungroup() %>%
  group_by(user_id) %>%
  dplyr::summarise(count_product=n()) %>%
  ungroup()
```
```{r}
ggplot(p2, aes(count_product)) + geom_histogram(binwidth = 10)+labs(title="Number of distinct products by user",
        x ="Number of Products", y = "Number of Users")+coord_cartesian(xlim = c(0, 250))
```

Number of distinct users/item
```{r}
p3 <- X %>%
  group_by(product_id,user_id) %>% 
  dplyr::summarise(count4=n()) %>% 
  select(user_id,product_id,count4) %>% 
  ungroup() %>%
  group_by(product_id) %>% 
  dplyr::summarise(count_user=n()) %>% 
  ungroup() 


ggplot(p3, aes(count_user)) + geom_histogram(binwidth = 50)+labs(title="Number of distinct users/item",
        x ="Number of Users", y = "Number of Products")+coord_cartesian(xlim = c(0, 500))
```
Most frequently bought products
```{r}
X %>% ggplot(aes(x= department)) +
  geom_histogram(stat="count", fill="green")+
  coord_flip()

```
Most orders come from the produce aisle, with snacks and dairy, eggs among the top 3 department aisles. 

'order_dow' is the day of week. Which days are orders more commonly placed on?
```{r}
X %>% ggplot(aes(x=order_dow))+
      geom_histogram()
#Sunday, Monday and Saturday appear to be the most common days where people place their orders.
```
How many days pass between an order and the next order?
```{r}
X %>% ggplot(aes(x=days_since_prior_order))+
      geom_histogram()
```
People most commonly order again after 30 days (1 month), or 7 days (1 week).

```{r}
  ggplot(X) +
geom_bar(mapping= aes(x=reordered))

```

Find the percentage of transactions from the top 10 products sold.
```{r}
X %>% 
  group_by(product_name) %>%
  dplyr::summarize(count =n()) %>%
  mutate(pct=(count/sum(count))*100) %>%
  arrange(desc(pct)) %>%
  ungroup() %>%
  top_n(10, wt=pct)

```
Top 10 products sold and the percentage of transactions they are involved in. Bananas make up 1.35% of the transactions, while organic bananas make up 1.1% of the transactions and organic strawberries make up 0.79% of the transactions. 


How many items are in each transaction?

```{r}
X %>%
  group_by(order_id) %>%
  dplyr::summarise(number_items=last(add_to_cart_order)) %>%
  ggplot(aes(x=number_items)) +
  geom_histogram(stat="count", fill="blue") +
  geom_rug()+
  coord_cartesian(xlim=c(0,80))

```


Items most often reordered:

```{r}

reordered <- X %>%
  group_by(product_name) %>%
  dplyr::summarize(proportion_reordered = mean(reordered), n=n()) %>%
  filter(n>40) %>%
  top_n(10, wt=proportion_reordered) %>%
  arrange(desc(proportion_reordered)) 

kable(reordered)
```




X_merge column is not needed for association rule mining, so can set to NULL:
```{r}
X$X_merge <- NULL
```
Need to convert dataframe to transaction data so that all items bought together in one order is in one row. Currently different products from the same order are in their own rows (singles format).

```{r}
library(plyr)
#ddply(dataframe, variables_to_be_used_to_split_data_frame, function_to_be_applied)

transactionData <- ddply(X, c("order_id","user_id"),
                       function(df1)paste(df1$product_name,     #paste is used to concatenate vectors to       characters
                       collapse = ","))     #collapse is used to separate the concatenated product names with a comma
```

Look at the transaction data. This is currently in the form of a basket format:
```{r, warning= FALSE, echo=FALSE, message=FALSE}
#transactionData
```


#set order id and user id to NULL in the transaction dataset since it will not be needed for item association
```{r}
transactionData$order_id <- NULL
transactionData$user_id <- NULL
```
rename column to items
```{r}
colnames(transactionData) <- c("items")
```

write the transaction data csv into a csv file:
```{r}
#write.csv(transactionData,"C:/Users/qt09n/Desktop/Project/market_basket_transactions.csv", quote = FALSE, row.names = FALSE)
```

take the transaction data file which is in basket format and convert it to an object of the transaction class
```{r, warning= FALSE, echo=FALSE, message=FALSE}
tr <- read.transactions('C:/Users/qt09n/Desktop/Project/market_basket_transactions.csv', format = 'basket', sep=',')
```
```{r}
tr
```
```{r}
summary(tr)
```

 131210 transactions (rows) and 50153 items (columns). 50153 is the product names. Density is the percentage of non-zero cells in a sparse matrix, which is the total number of items purchased divided by a possible number of items in that matrix. 
 
To calculate how many items were purchased: 131210 x 50153 x 0.00020449 = 1345662

A sparse matrix is a matrix in which most elements are zero.

Element (itemset/transaction) length distribution. This section is about how many transactions containing a certain number of items. The first row is the number of items in a transaction, and the second row is the number of transactions with that number of items. ie. There are 1877502617 transactions with only 1 item. There are 1980472390 transactions with 2 items. 


To generate an item Frequency Plot to view the distribution of objects basedon itemMatrix. 

Create an item frequency plot for the top 50 items.
```{r}
itemFrequencyPlot(tr, topN=20, type="absolute", col=brewer.pal(8, 'Pastel2'), main="Absolute Item Frequency Plot")
```
According to the frequency plot, the top 20 products bought in Instacart are banana, bag of organic bananas, organic strawberries, organic baby spinach, large lemon, organic avocado, organic hass avocado, strawberries, limes, organic raspberries, organic blueberries, organic whole milk, organic zucchini, organic cucumber, organic yellow onion, organic garlic, seedless red grapes, organic red onion, asparagus and organic grape tomatoes.

This plot shows absolute frequency which are independent numeric frequencies for each item. 

To look at relative frequencies (how many times an item appears in comparison to others):
```{r}
itemFrequencyPlot(tr, topN=20, type="relative", col=brewer.pal(8, 'Pastel2'), main="Relative Item Frequency Plot")


```
### Generating Rules

Mine the rules using APRIORI algorithm. 

```{r}
association.rules <- apriori(tr, parameter= list(supp=0.001, conf=0.8, maxlen=10))
```
The apriori will take tr as the transaction object to apply the rule mining. Parameters allow you to set min_sup and min_confidence and min confidence of 0.8, maximum of 10 items(maxlen).


```{r}
summary(association.rules)

```
set of 255 rules were generated from the apriori algorithm. 

to look at just the top 10 rules:
```{r}
inspect(association.rules[1:10])

```

136 transactions where customers who bought Mini and Mobile also bough Natural Artesian Water. 136 transactions where people who bought 1000 sheet Rolls also bought 1a Ply, and 136 transactions where people who bought 1000 Sheet Rolls also bought Bathroom tissue. 


### Limiting the number and size of rules

Setting the the conf value and maxlen parameter to higher values will give stronger rules. 
```{r}
shorter_association_rules <- apriori(tr, parameter = list(supp=0.001, conf=0.9, maxlen=5))

```
```{r}

summary(shorter_association_rules)
```
```{r}
inspect(shorter_association_rules[1:10])

```

To remove redundant rules
```{r}
subset.rules <- which(colSums(is.subset(association.rules, association.rules))>1)  #get subset rules in vector
length(subset.rules)
#which() - gives you the position of elements in the vector where value = TRUE
#colSums() - row and column sums for dataframes and numeric arrays
#is.subset() - find out if elements of one vector contain all elements of other vector
```
To remove the subset rules:
```{r}
subset.association.rules <- association.rules[-subset.rules] #remove subset rules

```

To find out what customers buy before buying a certain product, use the appearance option in the apriori command. ie. to find out what people buy before buying French baguettes:
```{r}
baguette.association.rules <- apriori(tr, parameter = list(supp=0.001, conf=0.8), appearance = list(default="lhs", rhs="French Baguettes"))
```
To find out how many customers buy French baguettes along with other items:
```{r}
inspect(head(baguette.association.rules))

```

To find out answer to "What other items did customers who bought X item also buy?" ...ie. for French baguettes again:
```{r}
baguette.association.rules <- apriori(tr, parameter = list(supp=0.001, conf=0.8), appearance = list(lhs="French Baguettes", default="rhs"))
```
Keep lhs as French Baguettes because you want to find out the probability of how many customers buy French baguettes with other items:
```{r}
inspect(head(baguette.association.rules))
```

Scatterplot
```{r}
#filter rules with confidence greater than 0.6 or 60%
subRules <- association.rules[quality(association.rules)$confidence>0.6]

#plot subrules
plot(subRules)
```

Rules with high lift have low support

Plot options:
rulesObject = rules object to be plotted
measure= measures for rule interestingness ie. support, confidence, lift or combination of these depending on method value
shading = measure used to color points( support, confidence, lift); default=lift
metho=visualization method to be used(scatterplot, 2 key plot, matrix3D)
```{r}
plot(subRules, method="two-key plot")
```

Two key plot has support on x axis and confidence on y-axis. It uses order for coloring. Order is the number of items in the rule.


### Interactive Scatterplot 

Users can hover over rules and see the quality measures(support, confidence and lift).
```{r}
plot(subRules)
```

Graph based methods: 
vertices are labeled with item names; item sets or rules are indicated with a second set of vertices:
arrows point from items to rule vertices = LHS; arrow from rule to an item = RHS. Size & color = interest measure.

To get the top 10 rules with highest confidence:
```{r}
top10subRules <- head(subRules, n= 10, by="confidence")
```

Make interactive plot with engine=htmlwidget parameter in plot
```{r}
# plot(top10subRules, method="graph", engine="htmlwidget")   #html widget can not be shown in pdf
plot(top10subRules, method="graph")
```
To export graphs for sets of association rules in GraphML format (which you can open with Gephi tool):
```{r}
saveAsGraph(head(subRules, n=1000, by="lift"), file="rules.graphml")
```


### Individual Rule Representation

This is Parallel Coordinates Plot, used to visualize products with items and types of sales:
RHS = consequent, which is item that is suggested for customers to buy:
positions are LHS, where 2 = most recent item; and 1=item previously bought
```{r}
#filter top 20 rules with highest lift:
subRules2 <-head(subRules, n=20, by="lift")
plot(subRules2, method="paracoord")

```
According to this plot, if when someone buys salsa, and "hot".., they are likely to buy Roja. 

If someone has mango, and pear baby food in their cart, they are likely to buy Happy Baby Spinach as well.


## Recommender Method 2: Weighted Alternating Least Squares with Implicit Feedback Data

```{r, echo=FALSE, message=FALSE, warning=FALSE}
#Import data

orders<-read.csv("orders.csv")
products<-read.csv("products.csv")
departments<-read.csv("departments.csv")
prior<-read.csv("order_products__prior.csv")
train<-read.csv("order_products__train.csv")
test<-read.csv("sample_submission.csv")
```

```{r, message=FALSE, echo=FALSE, warning=FALSE, include=FALSE}
#merge prior & order 

prior_order<-orders %>% 
  filter(eval_set=="prior") %>% 
    left_join(prior,orders,by=c("order_id")) %>% 
    left_join(products,by=c("product_id")) %>% 
  filter(reordered==1)
#rm(orders,prior)

head(prior_order)

```

```{r}
#Extract columns for matrix 
#transactions<-X[,c("user_id","product_id","order_id")]             #if using training orders data
transactions<-prior_order[,c("user_id","product_id","order_id")]    #using prior orders data


#selecting 8K users for modeling
all_users<-unique(transactions$user_id)
randm_users<-sample(all_users,8000L)

#selecting 16K products
all_products<-unique(transactions$product_id)
rand_products<-sample(all_products,16000L)



#final data for matrix
interactions<-transactions %>% 
                     filter(user_id %in%randm_users & product_id %in%rand_products)

#find the total orders for each user per product
interactions_sample<-interactions %>% 
  group_by(user_id,product_id) %>% 
  dplyr::summarise(orders=n())

```

```{r}
dim(interactions)
```


```{r }


#encoding users and products
user_enc <-  interactions_sample %>%
  distinct(user_id) %>%
 rowid_to_column()

names(user_enc)[names(user_enc) == "rowid"]<- "uid_enc"
  
product_enc<- interactions_sample %>%
  distinct(product_id)  %>%
 rowid_to_column() 

names(product_enc)[names(product_enc) == "rowid"]<- "pid_enc"

```


```{r}
#sparse matrix
data<-interactions_sample %>%
  inner_join(user_enc, by = 'user_id') %>% 
  inner_join(product_enc, by = 'product_id')



X = sparseMatrix(i = data$uid_enc, j = data$pid_enc, x = data$orders, 
                 dimnames = list(user_enc$user_id, product_enc$product_id))
```


```{r}
#test set

n_test <- 2000L
test_uid <- sample(nrow(user_enc), n_test)

X_train <-  X[-test_uid, ]
X_test <-  X[test_uid, ]

```


```{r}
# Split our test set into "history" or "future"
temp = as(X_test, "TsparseMatrix")
temp = data.table(i = temp@i, j = temp@j, x = temp@x) 



temp <- temp %>%
  group_by(i) %>%                         # group by user
  mutate(ct = length(j),                  # number of products each user has
         history = 
           sample(c(TRUE, FALSE), ct, replace = TRUE, prob = c(.5, .5))) %>%
  select(-ct)


X_test_history <- temp %>% filter(history == TRUE)
X_test_future <- temp %>% filter(history == FALSE)




X_test_history <- sparseMatrix(i = X_test_history$i,
                               j = X_test_history$j,
                               x = X_test_history$x,
                               dims = dim(X_test),
                               dimnames = dimnames(X_test),
                               index1 = FALSE)



X_test_future <- sparseMatrix(i = X_test_future$i,
                              j = X_test_future$j,
                              x = X_test_future$x,
                              dims = dim(X_test),
                              dimnames = dimnames(X_test),
                              index1 = FALSE)

```

```{r}
#  confidence functions and create matrices

lin_conf <- function(x, alpha) {
  x_confidence <- x
  stopifnot(inherits(x, "sparseMatrix"))
  x_confidence@x = 1 + alpha * x@x
  return(x_confidence)
}
```

```{r}

alpha <- .1
lambda <- 10
components <- 10L

#factor matrices for train and test

X_train_conf <- lin_conf(X_train, alpha)
X_test_history_conf <- lin_conf(X_test_history, alpha)
```

```{r}
# Initialize a model
# Define hyper parameters
#rank=the number of latent factors in the model (defaults to 10)
#value of alpha is calculated using cross validation





model <- WRMF$new(rank = components,
                  lambda = lambda,
                  feedback = 'implicit',
                  solver = 'conjugate_gradient')

```


```{r}

# Calculate user factors
train_user_factors <- model$fit_transform(X_train_conf)

# Products matrix and recommendations are  made by selecting the top 10  items for which P(ui) is greatest for an user.

test_predictions <- model$predict(X_test_history_conf, k = 10)


#Loss and Score or fixed product factors
trace = attr(train_user_factors, "trace")
ggplot(trace) +
  geom_line(aes(x = iter, y = value, col = scorer)) +
  labs(title = "Loss and Scoring Metrics by iteration") +
  theme(plot.title = element_text(hjust = .5))

```





## Recommenderlab for Evaluation of Different Recommender Algorithms in R

Using Recommederlab in R: 2 types of rating matrix for modelling is available; we will be using the binary rating matrix type where 0 indicates product is not purchased, while 1 indicates product is purchased. 

Binary rating matrix is useful when no actual user ratings is available, and it also does not require normalisation.



The rating matrix must be rearranged with orders in rows and products in columns. 

```{r}
X <- read.csv("C:/Users/qt09n/Desktop/Project/orders_TRAIN_products_MERGED.csv")
```

```{r include=FALSE}

#top 50 products train

top_products_train <- X %>% 
 group_by(user_id,product_id) %>% 
 dplyr::summarise(total=n()) %>% 
  ungroup() %>% 
  group_by(product_id) %>% 
  dplyr::summarise(count1=n()) %>% 
  ungroup() %>% 
arrange(desc(count1)) %>% 
top_n(50)

#top_products_train

```

```{r}
# train users for top 50  

train_users<- X %>%
  filter(product_id %in% top_products_train$product_id) %>% 
 group_by(user_id,product_id, .groups='keep') %>% 
  dplyr::summarise(tot=n()) %>% 
  ungroup() %>% 
  group_by(user_id) %>% 
  dplyr::summarise(count2=n()) %>% 
 arrange(desc(count2)) 
```
```{r}
View(train_users)
```

#transaction for top 50 products (train)
```{r}
train_top_50 <- X %>% 
filter(user_id %in%train_users$user_id  & product_id %in% top_products_train$product_id)
```

```{r}
dim(train_top_50)
```

Check if products are ordered multiple times within the same transaction:

```{r}
retail <- train_top_50 %>%
  #create a unique identifier for each product in a transaction using the order id and product name info:
  mutate(orderID_product = paste(order_id, product_name, sep=' '))
#225084 entries
```

```{r}
#filter out duplicates and drop unique identifier
retail <- retail[!duplicated(retail$orderID_product), ] %>%
  select(-orderID_product)
#still 225084 entries, so customers generally do not buy multiples of a single product within the same transaction
```

```{r}
ratings_matrix <- retail %>%
  select(order_id, product_name) %>%
  mutate(value=1) %>%                       #add a column of 1s
  spread(product_name, value, fill=0) %>%   #spread into user-item format
  select(-order_id) %>%
  as.matrix() %>%
  as("binaryRatingMatrix")                  #convert to recommenderlab class binary matrix
  
ratings_matrix
```
### Evaluation scheme and Model validation

Evaluate the model's effectiveness using recommenderlab's evaluation schemes.

Split the data into a training set and test set with train taking 80% of the data and test taking 20% of the data.

Set method="cross" and k=5 for 5 fold cross-validation. Data will be split into k subsets of equal size, and 80% of data will be used for training and last 20% for evaluation. Models are then estimated recursively 5 times, and a different train/test split is used each time. Results are then averaged to produce a single evaluation set.


```{r, warning= FALSE}
scheme <- ratings_matrix %>%
  evaluationScheme(method="cross",
                   k = 5,
                   train = 0.8,
                   given = -1)   #selecting given = -1 means that for the test users 'all but 1" randomly selected item is withheld for evaluation. 

scheme
```

### Set up a list of algorithms 

Create a list of algorithms from recommenderlab  and specify model parameters.  Consider schmemes which evaluate on the binary rating matrix and include random items algorithm for benchmarking.

```{r}
algorithms <- list(
  "association rules" = list(name = "AR",
                             param = list(supp=0.001, conf=0.01)),
  "random items"  = list(name = "RANDOM", param=NULL),                #randomly chosen items for comparison
  "popular items" = list(name = "POPULAR", param = NULL),             #recommender based on item popularity
  "item-based CF" = list(name = "IBCF", param = list(k=5))           #recommender based on item-based collaborative filtering
  #"user-based CF" = list(name = "UBCF",                               #recommender based on user-based collabortive filtering
  #                      param = list(method = "Cosine", nn= 50))      #user-based CF often runs into memory problems
  )

```

Pass the scheme and algorithms to the evaluate() function, to evaluate several recommender algorithms using an evaluation scheme. The end product is a evaluation result list. 

elect type= topNList to evaluate a Top N List of product recommendations and specify how many recommendations to calculate with the parameter n = c(1,3,5,10,15,20)

Note: for the UBCF method, it is important to allocate enough memory in RStudio for processing. 

To check the current limit in R session use memory.limit(); 
and then to increase the size of memory use memory.limit(size=n)
ie. memory.limit(size=56000)

```{r}
#run garbage collection to free up memory for analysis:
gc()

#remove from global environment variables which are not needed for the analysis:
# rm(X, transactionData, train_users, tr, top10subRules, top_products_train, subset.association.rules, subRules2, subRules, shorter_association_rules, df, baguette.association.rules)


#change memory limit to 56000 Mb to prevent problems with memory limit ("cannot allocate vector of size 8.0 Gb")
memory.limit(size=56000)       

results <- recommenderlab::evaluate(scheme, 
                                    algorithms,
                                    type= "topNList",
                                    n = c(1,3,5,10, 15, 20))

```
```{r}
results
```


```{r}
names(results)

#Access individual results by list subsetting using an index or the name specified when calling evaluate().
results[["item-based CF"]]
```

### Visualise the Results 

Use plot function from recommenderlab to compare model performance

Arrange confusion matrix for one model in a convenient format:


```{r}
plot(results, annotate=c(1,3), legend="bottomright")

```
Comparison of ROC curves. For this datset and the given evaluation scheme, popular items and association rules outperform the other methods, in providing a better combination of TPR and FPR amongst the 4 alogorithms evaluated for the top-N list recommendations.




```{r}
plot(results, "prec/rec", annotate=3, legend="topright")

```
Comparison of precision vs. recall curves for the 4 recommender algorithms shows that Popular items and association rules performed the best for the given evaluation scheme. 







## References

https://towardsdatascience.com/market-basket-analysis-with-recommenderlab-5e8bdc0de236

Michael Hahsler (2021). recommenderlab: Lab for Developing and
Testing Recommender Algorithms. R package version 0.2-7.
https://github.com/mhahsler/recommenderlab

https://www.datacamp.com/community/tutorials/market-basket-analysis-r#code


https://datascienceplus.com/a-gentle-introduction-on-market-basket-analysis%E2%80%8A-%E2%80%8Aassociation-rules/

https://en.wikipedia.org/wiki/Sparse_matrix

https://cran.r-project.org/web/packages/arulesViz/vignettes/arulesViz.pdf

https://github.com/willsmorgan/Recommender-Systems-using-W-ALS/blob/master/W-ALS%20Final.pdf


